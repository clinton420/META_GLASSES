{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0351f07-1ca9-46ab-a199-78c2e44a9077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e6d9fb017e4b77994c84362257baf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:2026\n",
      " * Running on http://192.168.0.13:2026\n",
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:2025\n",
      " * Running on http://192.168.0.13:2025\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "66.207.195.70 - - [24/Nov/2024 16:37:45] \"POST /receive HTTP/1.1\" 200 -\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "66.207.195.70 - - [24/Nov/2024 16:38:02] \"POST /receive HTTP/1.1\" 200 -\n",
      "66.207.195.70 - - [24/Nov/2024 16:38:16] \"\u001b[31m\u001b[1mPOST /receive HTTP/1.1\u001b[0m\" 400 -\n",
      "66.207.195.70 - - [24/Nov/2024 16:38:29] \"\u001b[31m\u001b[1mPOST /receive HTTP/1.1\u001b[0m\" 400 -\n",
      "66.207.195.70 - - [24/Nov/2024 16:38:36] \"POST /receive HTTP/1.1\" 200 -\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "66.207.195.70 - - [24/Nov/2024 16:39:13] \"POST /receive HTTP/1.1\" 200 -\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "66.207.195.70 - - [24/Nov/2024 16:39:28] \"POST /receive HTTP/1.1\" 200 -\n",
      "66.207.195.70 - - [24/Nov/2024 16:39:44] \"POST /receive HTTP/1.1\" 200 -\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "66.207.195.70 - - [24/Nov/2024 16:43:22] \"POST /receive HTTP/1.1\" 200 -\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "66.207.195.70 - - [24/Nov/2024 16:44:38] \"POST /receive HTTP/1.1\" 200 -\n",
      "66.207.195.70 - - [24/Nov/2024 16:45:37] \"\u001b[31m\u001b[1mPOST /receive HTTP/1.1\u001b[0m\" 400 -\n",
      "66.207.195.70 - - [24/Nov/2024 16:46:12] \"\u001b[31m\u001b[1mPOST /receive HTTP/1.1\u001b[0m\" 400 -\n",
      "66.207.195.70 - - [24/Nov/2024 16:47:57] \"\u001b[31m\u001b[1mPOST /receive HTTP/1.1\u001b[0m\" 400 -\n",
      "66.207.195.70 - - [24/Nov/2024 16:48:27] \"POST /receive HTTP/1.1\" 200 -\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "66.207.195.70 - - [24/Nov/2024 16:49:12] \"POST /receive HTTP/1.1\" 200 -\n",
      "66.207.195.70 - - [24/Nov/2024 16:50:27] \"\u001b[31m\u001b[1mPOST /receive HTTP/1.1\u001b[0m\" 400 -\n",
      "66.207.195.70 - - [24/Nov/2024 16:50:40] \"\u001b[31m\u001b[1mPOST /receive HTTP/1.1\u001b[0m\" 400 -\n",
      "66.207.195.70 - - [24/Nov/2024 16:51:40] \"\u001b[31m\u001b[1mPOST /receive HTTP/1.1\u001b[0m\" 400 -\n",
      "66.207.195.70 - - [24/Nov/2024 16:51:57] \"\u001b[31m\u001b[1mPOST /receive HTTP/1.1\u001b[0m\" 400 -\n",
      "66.207.195.70 - - [24/Nov/2024 16:52:22] \"\u001b[31m\u001b[1mPOST /receive HTTP/1.1\u001b[0m\" 400 -\n",
      "66.207.195.70 - - [24/Nov/2024 16:53:29] \"\u001b[31m\u001b[1mPOST /receive HTTP/1.1\u001b[0m\" 400 -\n",
      "66.207.195.70 - - [24/Nov/2024 16:53:40] \"\u001b[31m\u001b[1mPOST /receive HTTP/1.1\u001b[0m\" 400 -\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "66.207.195.70 - - [24/Nov/2024 16:54:09] \"POST /receive HTTP/1.1\" 200 -\n",
      "66.207.195.70 - - [24/Nov/2024 17:17:52] \"\u001b[31m\u001b[1mPOST /receive HTTP/1.1\u001b[0m\" 400 -\n",
      "66.207.195.70 - - [24/Nov/2024 17:18:03] \"POST /receive HTTP/1.1\" 200 -\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "66.207.195.70 - - [24/Nov/2024 17:19:50] \"POST /receive HTTP/1.1\" 200 -\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "66.207.195.70 - - [24/Nov/2024 17:20:45] \"POST /receive HTTP/1.1\" 200 -\n",
      "66.207.195.70 - - [24/Nov/2024 17:21:19] \"POST /receive HTTP/1.1\" 200 -\n",
      "66.207.195.70 - - [24/Nov/2024 17:24:34] \"OPTIONS /receive HTTP/1.1\" 200 -\n",
      "66.207.195.70 - - [24/Nov/2024 17:24:35] \"\u001b[33mPROPFIND /receive/ HTTP/1.1\u001b[0m\" 404 -\n",
      "66.207.195.70 - - [24/Nov/2024 17:24:35] \"\u001b[33mPROPFIND /receive/ HTTP/1.1\u001b[0m\" 404 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import threading\n",
    "import json\n",
    "\n",
    "# Initialize Flask apps\n",
    "receive_app = Flask(__name__)\n",
    "llama_app = Flask(__name__)\n",
    "\n",
    "# Load LLaMA model and tokenizer\n",
    "model_path = \"./Test1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16).eval()\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Move model to GPU(s) if available\n",
    "if torch.cuda.is_available():\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    model = model.cuda()\n",
    "\n",
    "# Store the conversation state and responses\n",
    "conversation_state = {\n",
    "    \"current_state\": \"waiting_for_question\",\n",
    "    \"current_question\": \"\",\n",
    "    \"current_answer\": \"\",\n",
    "    \"current_mcq\": None,\n",
    "    \"correct_answer\": None\n",
    "}\n",
    "\n",
    "def clean_topic(question):\n",
    "    \"\"\"Clean the question to get a proper topic.\"\"\"\n",
    "    # Remove question words and common patterns\n",
    "    question = question.lower()\n",
    "    question = question.replace(\"what is\", \"\").replace(\"how does\", \"\").replace(\"why does\", \"\")\n",
    "    question = question.replace(\"explain\", \"\").replace(\"describe\", \"\")\n",
    "    # Remove question marks\n",
    "    question = question.replace(\"?\", \"\")\n",
    "    # Clean up extra spaces\n",
    "    question = \" \".join(question.split())\n",
    "    return question.strip()\n",
    "\n",
    "def generate_mcq(question, answer):\n",
    "    try:\n",
    "        # Clean the topic for better MCQ generation\n",
    "        topic = clean_topic(question)\n",
    "        \n",
    "        mcq_prompt = f\"\"\"Create an educational multiple choice question based on this content:\n",
    "\n",
    "Topic: {topic}\n",
    "Detailed Content: {answer}\n",
    "\n",
    "Generate 4 options where only one is correct. Format exactly like this:\n",
    "\n",
    "{{\n",
    "    \"question\": \"What is the primary function of the {topic}?\",\n",
    "    \"options\": {{\n",
    "        \"A\": \"A complete, accurate statement about the {topic}\",\n",
    "        \"B\": \"A plausible but incorrect statement about the {topic}\",\n",
    "        \"C\": \"Another plausible but incorrect statement about the {topic}\",\n",
    "        \"D\": \"Another plausible but incorrect statement about the {topic}\"\n",
    "    }},\n",
    "    \"correct_answer\": \"A\",\n",
    "    \"explanation\": \"The correct answer is A because [explanation based on the content]\"\n",
    "}}\n",
    "\n",
    "Rules:\n",
    "1. All options must be complete, clear sentences\n",
    "2. Options must be directly related to {topic}\n",
    "3. Only one option should be correct\n",
    "4. Don't use 'all of the above' or 'none of the above'\n",
    "5. Make options similar in length\n",
    "6. Do not use placeholder text in the options\n",
    "\n",
    "Generate MCQ now:\"\"\"\n",
    "\n",
    "        mcq_inputs = tokenizer(mcq_prompt, return_tensors=\"pt\", padding=True).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        mcq_ids = model.module.generate(\n",
    "            **mcq_inputs,\n",
    "            max_new_tokens=500,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "        mcq_response = tokenizer.decode(mcq_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract and validate JSON\n",
    "        try:\n",
    "            start_idx = mcq_response.find('{')\n",
    "            end_idx = mcq_response.rfind('}') + 1\n",
    "            if start_idx != -1 and end_idx != -1:\n",
    "                json_str = mcq_response[start_idx:end_idx]\n",
    "                mcq_data = json.loads(json_str)\n",
    "                \n",
    "                if all(key in mcq_data for key in [\"question\", \"options\", \"correct_answer\"]):\n",
    "                    if isinstance(mcq_data[\"options\"], dict) and len(mcq_data[\"options\"]) == 4:\n",
    "                        return mcq_data\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "        \n",
    "        # Fallback MCQ generation\n",
    "        sentences = answer.split('.')\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        main_fact = sentences[0] if sentences else answer[:100]\n",
    "        \n",
    "        return {\n",
    "            \"question\": f\"Which statement about the {topic} is correct?\",\n",
    "            \"options\": {\n",
    "                \"A\": main_fact,\n",
    "                \"B\": f\"The {topic} is responsible for breaking down fats in the blood\",\n",
    "                \"C\": f\"The {topic} is only active during physical exercise\",\n",
    "                \"D\": f\"The {topic} plays no role in the human body's functioning\"\n",
    "            },\n",
    "            \"correct_answer\": \"A\",\n",
    "            \"explanation\": f\"The correct answer is A because {main_fact.lower()}\"\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating MCQ: {e}\")\n",
    "        raise\n",
    "\n",
    "def generate_response(message, context=\"\"):\n",
    "    try:\n",
    "        if conversation_state[\"current_state\"] == \"waiting_for_mcq_answer\":\n",
    "            # Verify MCQ answer\n",
    "            selected_option = message.upper()\n",
    "            correct_answer = conversation_state[\"correct_answer\"]\n",
    "            is_correct = selected_option == correct_answer\n",
    "            explanation = conversation_state[\"current_mcq\"].get(\"explanation\", \"\")\n",
    "            \n",
    "            if is_correct:\n",
    "                return f\"Correct! Well done! {explanation}\"\n",
    "            else:\n",
    "                return f\"That's not quite right. {explanation}\"\n",
    "        else:\n",
    "            # Normal question-answer\n",
    "            answer_prompt = f\"Question: {message}\\nProvide a detailed educational answer:\"\n",
    "            answer_inputs = tokenizer(answer_prompt, return_tensors=\"pt\", padding=True).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            answer_ids = model.module.generate(\n",
    "                **answer_inputs,\n",
    "                max_new_tokens=200,\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            answer = tokenizer.decode(answer_ids[0], skip_special_tokens=True)\n",
    "            return answer.split(\"Answer:\")[-1].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during response generation: {e}\")\n",
    "        return \"An error occurred while generating the answer.\"\n",
    "\n",
    "@receive_app.route('/receive', methods=['POST'])\n",
    "def receive_data():\n",
    "    try:\n",
    "        if not request.is_json:\n",
    "            return jsonify({\"error\": \"Request must be in JSON format\"}), 400\n",
    "\n",
    "        data = request.get_json()\n",
    "        message = data.get(\"message\")\n",
    "        \n",
    "        if not message:\n",
    "            return jsonify({\"error\": \"No 'message' provided\"}), 400\n",
    "\n",
    "        if conversation_state[\"current_state\"] == \"waiting_for_question\":\n",
    "            conversation_state[\"current_question\"] = message\n",
    "            answer = generate_response(message)\n",
    "            conversation_state[\"current_answer\"] = answer\n",
    "            conversation_state[\"current_state\"] = \"waiting_for_understanding\"\n",
    "            return jsonify({\n",
    "                \"response_type\": \"answer\",\n",
    "                \"answer\": answer,\n",
    "                \"instruction\": \"Please reply with 'i got it' when you understand the answer.\"\n",
    "            })\n",
    "\n",
    "        elif conversation_state[\"current_state\"] == \"waiting_for_understanding\":\n",
    "            if \"i got it\" in message.lower():\n",
    "                try:\n",
    "                    mcq_data = generate_mcq(\n",
    "                        conversation_state[\"current_question\"],\n",
    "                        conversation_state[\"current_answer\"]\n",
    "                    )\n",
    "                    conversation_state[\"current_mcq\"] = mcq_data\n",
    "                    conversation_state[\"correct_answer\"] = mcq_data[\"correct_answer\"]\n",
    "                    conversation_state[\"current_state\"] = \"waiting_for_mcq_answer\"\n",
    "                    \n",
    "                    return jsonify({\n",
    "                        \"response_type\": \"mcq\",\n",
    "                        \"question\": mcq_data[\"question\"],\n",
    "                        \"options\": mcq_data[\"options\"],\n",
    "                        \"instruction\": \"Please select one option (A, B, C, or D)\"\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"MCQ generation error: {e}\")\n",
    "                    return jsonify({\"error\": f\"Failed to generate MCQ: {str(e)}\"}), 500\n",
    "            else:\n",
    "                return jsonify({\n",
    "                    \"error\": \"Please confirm your understanding by saying 'I got it'\"\n",
    "                }), 400\n",
    "\n",
    "        elif conversation_state[\"current_state\"] == \"waiting_for_mcq_answer\":\n",
    "            if message.upper() not in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "                return jsonify({\n",
    "                    \"error\": \"Please select a valid option (A, B, C, or D)\"\n",
    "                }), 400\n",
    "\n",
    "            result = generate_response(message, \"verify_mcq\")\n",
    "            conversation_state[\"current_state\"] = \"waiting_for_question\"  # Reset state\n",
    "            return jsonify({\n",
    "                \"response_type\": \"mcq_result\",\n",
    "                \"result\": result,\n",
    "                \"correct_answer\": conversation_state[\"correct_answer\"],\n",
    "                \"explanation\": conversation_state[\"current_mcq\"].get(\"explanation\", \"\"),\n",
    "                \"message\": \"You can ask another question now!\"\n",
    "            })\n",
    "\n",
    "        return jsonify({\"error\": \"Invalid state\"}), 400\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in /receive endpoint: {e}\")\n",
    "        return jsonify({\"error\": f\"An internal error occurred: {str(e)}\"}), 500\n",
    "\n",
    "@llama_app.route('/generate_response', methods=['POST'])\n",
    "def get_response():\n",
    "    try:\n",
    "        if not request.is_json:\n",
    "            return jsonify({\"error\": \"Request must be in JSON format\"}), 400\n",
    "\n",
    "        data = request.get_json()\n",
    "        message = data.get(\"message\", \"\")\n",
    "        \n",
    "        if not message:\n",
    "            return jsonify({\"error\": \"No 'message' provided\"}), 400\n",
    "\n",
    "        answer = generate_response(message)\n",
    "        return jsonify({\"answer\": answer})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in /generate_response endpoint: {e}\")\n",
    "        return jsonify({\"error\": \"An internal error occurred\"}), 500\n",
    "\n",
    "def run_receive_app():\n",
    "    receive_app.run(host='0.0.0.0', port=2025)\n",
    "\n",
    "def run_llama_app():\n",
    "    llama_app.run(host='0.0.0.0', port=2026)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    receive_thread = threading.Thread(target=run_receive_app)\n",
    "    llama_thread = threading.Thread(target=run_llama_app)\n",
    "    \n",
    "    receive_thread.start()\n",
    "    llama_thread.start()\n",
    "    \n",
    "    receive_thread.join()\n",
    "    llama_thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d97c5da-ce57-4273-b627-344e61eac5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf71ab6-bd3f-49c8-bada-e45af8dfca59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
